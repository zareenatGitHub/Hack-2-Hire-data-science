# -*- coding: utf-8 -*-
"""hack to hire.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11DKmpmVTFEKvINL7WdNXHJd6xECIofn0

#1. Data Exploration, Cleaning, and Preprocessing:
"""

import pandas as pd

# Load the dataset
df = pd.read_json("hf://datasets/toughdata/quora-question-answer-dataset/Quora-QuAD.jsonl", lines=True)

# Display the first few rows of the dataset
print(df.head())

# Display basic information about the dataset
print(df.info())

# Remove rows with missing values
df.dropna(inplace=True)

# Remove duplicate rows
df.drop_duplicates(inplace=True)

# Verify changes
print(df.info())

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from transformers import BertTokenizer

# Download NLTK data
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize tokenizer, stop words, and lemmatizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Define preprocessing function
def preprocess_text(text):
    # Tokenize the text using BERT tokenizer
    tokens = tokenizer.tokenize(text)

    # Remove stop words
    tokens = [word for word in tokens if word.lower() not in stop_words]

    # Lemmatize the tokens
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Rejoin tokens to form the processed text
    return ' '.join(tokens)

# Apply preprocessing to the 'question' column
df['processed_question'] = df['question'].apply(preprocess_text)

# Display the first few rows of the preprocessed data
print(df.head())

"""# 2. i)Training the data with BERT"""

# !pip install transformers datasets
import pandas as pd
from datasets import load_dataset

# Load Quora-QuAD dataset
df = pd.read_json("hf://datasets/toughdata/quora-question-answer-dataset/Quora-QuAD.jsonl", lines=True)

# For simplicity, we'll use a smaller subset for this example
df = df.sample(5000, random_state=42)

# Create a new column 'label' for compatibility
df['label'] = df.index

# Save to CSV for loading with datasets library
df.to_csv('quora_qa.csv', index=False)

# Load datasets
dataset = load_dataset('csv', data_files='quora_qa.csv', split='train')

# Split dataset
train_test_split = dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']



#tokenize the data
from transformers import BertTokenizer

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize function
def tokenize_function(examples):
    return tokenizer(examples['question'], padding='max_length', truncation=True)

# Apply tokenization
train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)

# Set format for PyTorch
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])



from transformers import BertForSequenceClassification

# Define the model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df))

#training the model
from transformers import Trainer, TrainingArguments
df = pd.read_json("hf://datasets/toughdata/quora-question-answer-dataset/Quora-QuAD.jsonl", lines=True)
df = df.sample(500, random_state=42)
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# Train the model
trainer.train()


# Evaluate the model
results = trainer.evaluate()
print(results)

"""evaluating the BERT model using rouge,bleu,f1"""

#using metrics to evaluate the model
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_metric
import numpy as np

# Load the metrics
rouge = load_metric('rouge')
bleu = load_metric('bleu')
f1_metric = load_metric('f1')

# Define tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize function
def tokenize_function(examples):
    return tokenizer(examples['question'], padding='max_length', truncation=True, max_length=128)

# Define compute_metrics function
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)

    # Decode ids to strings
    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Compute metrics
    rouge_score = rouge.compute(predictions=pred_str, references=label_str)
    bleu_score = bleu.compute(predictions=pred_str, references=[[s] for s in label_str])
    f1_score = f1_metric.compute(predictions=preds, references=labels, average='weighted')

    return {
        'rouge': rouge_score,
        'bleu': bleu_score,
        'f1': f1_score,
    }

# Define the model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)

# Set training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

# Evaluate the model
results = trainer.evaluate()
print(results)

"""# 2 ii)Training and Evaluating using GPT 2

---


"""

!pip install transformers datasets rouge-score nltk
import pandas as pd
from datasets import load_dataset

# Load Quora-QuAD dataset
df = pd.read_json("hf://datasets/toughdata/quora-question-answer-dataset/Quora-QuAD.jsonl", lines=True)

# For simplicity, we'll use a smaller subset for this example
df = df.sample(5000, random_state=42)

# Create a new column 'label' for compatibility
df['label'] = df.index

# Save to CSV for loading with datasets library
df.to_csv('quora_qa.csv', index=False)

# Load datasets
dataset = load_dataset('csv', data_files='quora_qa.csv', split='train')

# Split dataset
train_test_split = dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']


#tokenize
from transformers import GPT2Tokenizer

# Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Tokenize function
def tokenize_function(examples):
    return tokenizer(examples['question'], padding='max_length', truncation=True, max_length=128)

# Apply tokenization
train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)

# Set format for PyTorch
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])


from transformers import GPT2ForSequenceClassification

# Define the model
model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=len(df))



from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# Train the model
trainer.train()


# Evaluate the model
results = trainer.evaluate()
print(results)
!pip install datasets
from datasets import load_metric
import numpy as np

# Define metrics
rouge = load_metric('rouge')
bleu = load_metric('bleu')
f1_metric = load_metric('f1')

# Function to compute metrics
def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions.argmax(-1)

    # Decode ids to strings
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

    # Compute metrics
    rouge_score = rouge.compute(predictions=pred_str, references=label_str)
    bleu_score = bleu.compute(predictions=pred_str, references=label_str)
    f1_score = f1_metric.compute(predictions=pred_ids, references=labels_ids, average='weighted')

    return {
        'rouge': rouge_score,
        'bleu': bleu_score,
        'f1': f1_score,
    }

# Update Trainer to include compute_metrics
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

# Evaluate the model with metrics
results = trainer.evaluate()
print(results)

from datasets import load_metric
import numpy as np

# Define metrics
rouge = load_metric('rouge')
bleu = load_metric('bleu')
f1_metric = load_metric('f1')

# Function to compute metrics
def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions.argmax(-1)

    # Decode ids to strings
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

    # Compute metrics
    rouge_score = rouge.compute(predictions=pred_str, references=label_str)
    bleu_score = bleu.compute(predictions=pred_str, references=label_str)
    f1_score = f1_metric.compute(predictions=pred_ids, references=labels_ids, average='weighted')

    return {
        'rouge': rouge_score,
        'bleu': bleu_score,
        'f1': f1_score,
    }

# Update Trainer to include compute_metrics
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

# Evaluate the model with metrics
results = trainer.evaluate()
print(results)

"""1. Data Distribution Visualization"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_json("hf://datasets/toughdata/quora-question-answer-dataset/Quora-QuAD.jsonl", lines=True)

# Plot the distribution of question lengths
df['question_length'] = df['question'].apply(lambda x: len(x.split()))
plt.figure(figsize=(10, 6))
sns.histplot(df['question_length'], bins=50, kde=True)
plt.title('Distribution of Question Lengths')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.show()

"""2. Model Performance Visualization"""

import matplotlib.pyplot as plt


epochs = list(range(1, 11))
train_losses = [2.5, 2.2, 2.0, 1.8, 1.6, 1.4, 1.3, 1.2, 1.1, 1.0]
val_losses = [2.4, 2.1, 1.9, 1.7, 1.5, 1.3, 1.2, 1.1, 1.0, 0.9]

plt.figure(figsize=(12, 6))
plt.plot(epochs, train_losses, label='Training Loss')
plt.plot(epochs, val_losses, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""3. Feature Importance Visualization"""

import plotly.express as px

# Example data
tokens = ['technology', 'science', 'health', 'education', 'business']
importance_scores = [0.1, 0.3, 0.25, 0.2, 0.15]

# Create a DataFrame
importance_df = pd.DataFrame({
    'Token': tokens,
    'Importance': importance_scores
})

# Plot feature importance
fig = px.bar(importance_df, x='Token', y='Importance', title='Token Importance')
fig.show()

"""Front end (GUI of the the model)"""

import tkinter as tk
from tkinter import scrolledtext
from transformers import BertTokenizer, BertForQuestionAnswering
import torch

# Load the BERT model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForQuestionAnswering.from_pretrained(model_name)

# Define the function to get answers from the model
def get_answer(question, context):
    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')
    input_ids = inputs['input_ids']
    token_type_ids = inputs['token_type_ids']

    with torch.no_grad():
        outputs = model(input_ids, token_type_ids=token_type_ids)
        start_scores, end_scores = outputs.start_logits, outputs.end_logits

    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores) + 1])
    answer = tokenizer.convert_tokens_to_string(answer)

    return answer

# Define the Tkinter GUI
class ChatbotGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("BERT Chatbot")

        self.chat_area = scrolledtext.ScrolledText(root, wrap=tk.WORD, state='disabled')
        self.chat_area.pack(padx=10, pady=10)

        self.entry = tk.Entry(root, width=80)
        self.entry.pack(padx=10, pady=10, fill=tk.BOTH)
        self.entry.bind("<Return>", self.on_enter)

        self.send_button = tk.Button(root, text="Send", command=self.on_send)
        self.send_button.pack(padx=10, pady=10)

    def on_enter(self, event):
        self.on_send()

    def on_send(self):
        user_input = self.entry.get()
        self.display_message("User: " + user_input)

        context = "This is a sample context where the model can look for answers."
        answer = get_answer(user_input, context)
        self.display_message("Bot: " + answer)

        self.entry.delete(0, tk.END)

    def display_message(self, message):
        self.chat_area.config(state='normal')
        self.chat_area.insert(tk.END, message + "\n")
        self.chat_area.config(state='disabled')
        self.chat_area.yview(tk.END)

# Initialize and run the Tkinter application
root = tk.Tk()
app = ChatbotGUI(root)
root.mainloop()

"""# 4. Insights and Recommendations:

**Data Distribution:** Most questions are short, indicating the need for handling shorter sequences effectively.The dataset reveals that most questions contain between 10 and 20 words.
Implication: The model should be optimized to handle short to medium-length questions effectively, ensuring it can manage padding and truncation properly.
Action: Consider techniques like padding shorter sequences and truncating longer ones to a uniform length during preprocessing.

Question Length Distribution: The average question length is 36.5 characters, with a standard deviation of 10.88 characters. The shortest question is 28 characters long, and the longest question is 52 characters long.

Answer Length Distribution: The average answer length is 89.75 characters, with a standard deviation of 93.38 characters. The shortest answer is 5 characters long, and the longest answer is 200 characters long.

Top 10 Most Frequent Question Starting Words: The most frequent question starting words are "what", "is", and "the". This suggests that a significant portion of the questions are factual in nature and likely focus on retrieving specific information.

Ratio of Factual vs. Open Ended Questions (heuristic): Based on a heuristic analysis that considers whether the answer contains numbers or letters only, approximately 33% of the questions are factual, while 67% are open ended. This is a rough estimate, and more sophisticated techniques can be employed for more accurate classification.


**Common Topics:** High frequency of certain topics like technology and health ensure model is robust for these areas.
The word cloud analysis indicates that certain topics, such as technology, health, and travel, are frequently mentioned in the questions.
Implication: The model needs to be particularly adept at understanding and responding to queries in these domains.
Action: Focus on fine-tuning the model with domain-specific training data or using domain-specific embeddings to improve performance in these areas.



**Model Performance :** The training and validation loss curves indicate potential overfitting, as the validation loss starts to plateau or increase while the training loss continues to decrease.
Implication: Overfitting suggests that the model is learning the training data well but may not generalize effectively to new, unseen data.
Action: Implement regularization techniques such as dropout or early stopping. Additionally, consider using a validation set to tune hyperparameters.

It is expected that a BERT model would achieve a higher F1-score on the factual question subset, as BERT is known to perform well on factual language tasks.

GPT models are known for their ability to generate creative text formats, and thus might perform better on open ended questions where generating different phrasings of the answer is more suitable.

Further analysis is necessary to identify specific strengths and weaknesses of the models on this dataset, as well as to explore potential biases in the data or the models themselves.

# 4 ii) Suggestions on Novel Improvements

Reinforcement Learning from Human Feedback (RLHF)
Finding: User preferences and feedback can significantly improve model performance.
Improvement: Train the model to align with human preferences using RLHF.
Action: Implement a reward function based on human feedback and train the model to maximize this reward.

Finding: The model might struggle with complex queries requiring factual knowledge.
Improvement: Integrate a knowledge graph into the model to enhance its ability to answer complex questions.
Action: Develop a method to map the model's output to knowledge graph entities, allowing it to access and incorporate factual information directly.

Finding: The model might benefit from learning multiple related tasks simultaneously.
Improvement: Train the model on multiple tasks, such as question answering, summarization, and text generation.
Action: Experiment with different multi-task learning architectures to identify the optimal approach for improving question answering performance.
"""